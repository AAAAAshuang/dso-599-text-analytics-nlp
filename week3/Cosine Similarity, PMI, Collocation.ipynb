{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity Example\n",
    "\n",
    "### Intro to Algorithmic Marketing:\n",
    "![alt text](images/cos-sim-textbook1.png \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Magnitude of a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First approach: 3.7416573867739413\n",
      "Second approach: 3.7416573867739413\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "def magnitude(x): \n",
    "    return math.sqrt(sum(i**2 for i in x))\n",
    "\n",
    "vectorA = [0,3,1,2]\n",
    "\n",
    "print(f\"First approach: {magnitude(vectorA)}\")\n",
    "print(f\"Second approach: {np.linalg.norm(vectorA)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Count Vectorization - One Hot Encoding and other Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe is (655, 23148)\n",
      "Total number of occurences: 402534\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "plots_df = pd.read_csv(\"movie_plots.csv\")\n",
    "\n",
    "# filter only for American movies\n",
    "plots_df = plots_df[plots_df[\"Origin/Ethnicity\"] == \"American\"]\n",
    "\n",
    " # traditional CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    " # use English stopwords, and use one-hot encoding\n",
    "#vectorizer = CountVectorizer(stop_words=\"english\", binary=True)\n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "#vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "#vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "# use English stopwords, and use one-hot encoding, and the word must appear in at least two of the movie plots\n",
    "# and keep only the top 200\n",
    "# vectorizer = CountVectorizer(stop_words=\"english\", binary=True, min_df=2, max_features=200) \n",
    "\n",
    "X = vectorizer.fit_transform(plots_df[\"Plot\"])\n",
    "\n",
    "vectorized_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "print(f\"Shape of dataframe is {vectorized_df.shape}\")\n",
    "print(f\"Total number of occurences: {vectorized_df.sum().sum()}\")\n",
    "#print(f\"Word counts: {vectorized_df.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + [\".\",'.', \",\",\":\", \"''\", \"'s\", \"'\", \"``\", \"(\", \")\", \"-\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "articles = [f\"bbcsport/football/00{i}.txt\" for i in range(1,10)]\n",
    "\n",
    "for article in articles:\n",
    "    article = open(article) # open each sports article\n",
    "    for line in article.readlines():\n",
    "        line = line.replace(\"\\n\", \"\") # replace the new line escape character\n",
    "        if len(line) > 0: # if the line is not empty, process it\n",
    "            line = [lemmatizer.lemmatize(token) for token in word_tokenize(line)] \n",
    "            documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_documents = []\n",
    "for doc in documents:\n",
    "    new_document = []\n",
    "    for word in doc:\n",
    "        if word.strip().lower() not in stopwords:\n",
    "            new_document.append(word)\n",
    "    new_documents.append(new_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Champions', 'League'),\n",
       " ('Manchester', 'United'),\n",
       " ('Cristiano', 'Ronaldo'),\n",
       " ('Van', 'Nistelrooy'),\n",
       " ('Wayne', 'Rooney'),\n",
       " ('Alex', 'Ferguson'),\n",
       " ('FA', 'Cup'),\n",
       " ('Ferguson', 'wa'),\n",
       " ('Gary', 'Neville'),\n",
       " ('Man', 'Utd'),\n",
       " ('Manchester', 'City'),\n",
       " ('Sir', 'Alex'),\n",
       " ('national', 'team'),\n",
       " ('wa', \"n't\"),\n",
       " ('23', 'minute'),\n",
       " ('BBC', 'Radio'),\n",
       " ('Blues', 'bos'),\n",
       " ('Carling', 'Cup'),\n",
       " ('Carroll', 'Gary'),\n",
       " ('City', 'Sunday'),\n",
       " ('City', 'best'),\n",
       " ('Cup', 'final'),\n",
       " ('Five', 'Live'),\n",
       " ('Gallas', 'would'),\n",
       " ('Gerrard', 'ha'),\n",
       " ('Goodison', 'Park'),\n",
       " ('Home', 'International'),\n",
       " ('International', 'series'),\n",
       " ('Jose', 'Mourinho'),\n",
       " ('Man', 'City')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(new_documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "It's important to identify a **context window** when analyzing co-occurence. In the image below, the context window size is 4 (2 tokens to either side of the target word):\n",
    "\n",
    "![alt text](images/context_window.png \"Logo Title Text 1\")\n",
    "\n",
    "For the purposes of the next section, we'll define the **entire document as the context window.**\n",
    "\n",
    "Pointwise mutual information measures the ratio between the **joint probability of two events happening** with the probabilities of the two events happening, assuming they are independent. It can be defined with the following equation:\n",
    "\n",
    "$$\n",
    "PMI_{A,B} = log\\frac{p(A,B)}{p(A)p(B)}\n",
    "$$\n",
    "\n",
    "Remember that when two events are independent, $P(i,j) = P(i)P(j)$. Using PMI to just a raw word count is often preferable because very common words have extreme skew (\"the\" and \"of\" will co-occur frequently in the same  )\n",
    "\n",
    "```python\n",
    "import math\n",
    "def pmi(tokenA, tokenB, documents, word_counts):\n",
    "    \n",
    "    # word_counts[token_A] => number of times tokenA appears in the documents\n",
    "    # float(len(documents)) = number of documents\n",
    "    \n",
    "    prob_A = word_counts[tokenA] / float(len(documents))\n",
    "    prob_B = word_counts[tokenB] / float(len(documents))\n",
    "    prob_A_B = bigram_freq[\" \".join([word1, word2])] / float(len(documents))\n",
    "    return math.log(prob_A_B/float(prob_A*prob_B),2) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term Frequency / Inverse Document Frequency\n",
    "\n",
    "\n",
    "## Term Frequency\n",
    "![alt text](images/tf-idf1.png \"Term Frequency\")\n",
    "\n",
    "## Inverse Document Frequency\n",
    "![alt text](images/tf-idf2.png \"Inverse Document Frequency\")\n",
    "\n",
    "### Example Calculation\n",
    "\n",
    "![alt text](images/tf-idf4.png \"Example\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
