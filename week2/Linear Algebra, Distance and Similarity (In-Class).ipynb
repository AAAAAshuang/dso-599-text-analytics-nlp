{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "\n",
    "Links: https://www.fastcompany.com/90205359/google-you-auto-complete-me\n",
    "\n",
    "## Dot Products\n",
    "\n",
    "A dot product is defined as\n",
    "\n",
    "$ a \\cdot b = \\sum_{i}^{n} a_{i}b_{i} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + \\dots + a_{n}b_{n}$\n",
    "\n",
    "The geometric definition of a dot product is \n",
    "\n",
    "$ a \\cdot b = $\\|\\|b\\|\\|\\|\\|a\\|\\|\n",
    "\n",
    "### What does a dot product conceptually mean?\n",
    "\n",
    "A dot product is a representation of the similarity between two components, because it is calculated based upon shared elements.\n",
    "\n",
    "\n",
    "The actual value of a dot product reflects the direction of change:\n",
    "\n",
    "* **Zero**: we don't have any growth in the original direction\n",
    "* **Positive** number: we have some growth in the original direction\n",
    "* **Negative** number: we have negative (reverse) growth in the original direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4480736161291701"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [0,2]\n",
    "B = [0,1]\n",
    "\n",
    "def dot_product(x,y):\n",
    "    return sum(a*b for a,b in zip(x,y))\n",
    "\n",
    "dot_product(A,B)\n",
    "# What will the dot product of A and B be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1,2]\n",
    "B = [2,4]\n",
    "# What will the dot product of A and B be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'does', 'football', 'games', 'john', 'like', 'likes', 'mary', 'movies', 'much', 'not', 'to', 'too', 'watch']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Models\n",
    "\n",
    "You can use **`sklearn.feature_extraction.text.CountVectorizer`** to easily convert your corpus into a bag of words matrix:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "```\n",
    "Note that the output `X` here is not your traditional Numpy matrix! Calling **`type(X)`** here will yield **`<class 'scipy.sparse.csr.csr_matrix'>`**, which is a **CSR ([compressed sparse row format matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html))**. To convert it into an actual matrix, call the `toarray()` method:\n",
    "\n",
    "```python\n",
    "X.toarray()\n",
    "```\n",
    "Your output will be \n",
    "\n",
    "```\n",
    "array([[0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1],\n",
    "       [1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)\n",
    "```\n",
    "Notice that using **`X.shape`** $\\rightarrow$ `(2,14)`, indicating a total vocabulary size $V$ of 14. To get what word each of the 14 columns corresponds to, use **`vectorizer.get_feature_names()`**:\n",
    "```\n",
    "['also', 'does', 'football', 'games', 'john', 'like', 'likes', 'mary', 'movies', 'much', 'not', 'to', 'too', 'watch']\n",
    "```\n",
    "\n",
    "Notice, however, that as the vocabulary size $V$ increases, the percent of the matrix taken up by zero values increases:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits.\"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bag of words feature space is 72.63% sparse. \n",
      "That's approximately 2777.34 bytes of wasted memory.\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "        \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits. \"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "\n",
    "zeroes = np.where(X.flatten() == 0)[0].size \n",
    "percent_sparse = zeroes / X.size\n",
    "print(f\"The bag of words feature space is {round(percent_sparse * 100,2)}% sparse. \\n\\\n",
    "That's approximately {round(getsizeof(X) * percent_sparse,2)} bytes of wasted memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measures\n",
    "\n",
    "\n",
    "## Euclidean Distance\n",
    "\n",
    "Euclidean distances can range from 0 (completely identically) to $\\infty$ (extremely dissimilar). **Magnitude** plays an extremely important role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    " \n",
    "def euclidean_distance_1(x,y):\n",
    "    distance = sum((a-b)**2 for a, b in zip(x, y))\n",
    "    return sqrt(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's typically an easier way to write this function that takes advantage of Numpy's vectorization capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def euclidean_distance_2(x,y):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return np.linalg.norm(x-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures\n",
    "\n",
    "Similarity measures will always range between -1 and 1. A similarity of -1 means the two objects are complete opposites, while a similarity of 1 indicates the objects are identical.\n",
    "\n",
    "## Pearson Correlation Coefficient\n",
    "* We use **ρ** when the correlation is being measured from the population, and **r** when it is being generated from a sample.\n",
    "* An r value of 1 represents a **perfect linear** relationship, and a value of -1 represents a perfect inverse linear relationship.\n",
    "\n",
    "The equation for Pearson's correlation coefficient is \n",
    "$$\n",
    "ρ_{Χ_Υ} = \\frac{cov(X,Y)}{σ_Xσ_Y}\n",
    "$$\n",
    "\n",
    "### Intuition Behind Pearson Correlation Coefficient\n",
    "\n",
    "#### When $ρ_{Χ_Υ} = 1$ or  $ρ_{Χ_Υ} = -1$\n",
    "\n",
    "This requires **$cov(X,Y) = σ_Xσ_Y$** or **$-1 * cov(X,Y) = σ_Xσ_Y$** (in the case of $ρ = -1$) . This corresponds with all the data points lying perfectly on the same line.\n",
    "![Correlations](images/correlation.png \"Visualization of various r values for Pearson correlation coefficient\")\n",
    "\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "The cosine similarity of two vectors (each vector will usually represent one document) is a measure that calculates $ cos(\\theta)$, where $\\theta$ is the angle between the two vectors.\n",
    "\n",
    "Therefore, if the vectors are **orthogonal** to each other (90 degrees), $cos(90) = 0$. If the vectors are in exactly the same direction, $\\theta = 0$ and $cos(0) = 1$.\n",
    "\n",
    "Cosine similiarity **does not care about the magnitude of the vector, only the direction** in which it points. This can help normalize when comparing across documents that are different in terms of word count.\n",
    "\n",
    "### Shift Invariance\n",
    "\n",
    "* The Pearson correlation coefficient between X and Y does not change with you transform $X \\rightarrow a + bX$ and $Y \\rightarrow c + dY$, assuming $a$, $b$, $c$, and $d$ are constants and $b$ and $d$ are positive.\n",
    "* Cosine similarity does, however, change when transformed in this way.\n",
    "\n",
    "\n",
    "<h1><span style=\"background-color: #FFFF00\">Exercise (20 minutes):</span></h1>\n",
    "\n",
    ">In Python, find the **cosine similarity** and the **Pearson correlation coefficient** of the two following sentences, assuming a **one-hot encoded binary bag of words** model. You may use a library to create the BoW feature space, but do not use libraries other than `numpy` or `scipy` to compute Pearson, Spearman, and cosine similarity:\n",
    "\n",
    ">`A = \"John likes to watch movies. Mary likes movies too\"`\n",
    "\n",
    ">`B = \"John also likes to watch football games, but he likes to watch movies on occasion as well\"`\n",
    "\n",
    ">Your colleagues are having an intense debate about the methodology. The first document another data analyst Tony, says that it doesn't matter if you compare two documents of different lengths. **\"Since cosine similarity doesn't care about magnitude, it won't make any real impact,\"** he claims. But your manager Bhavna says there is no way this is true and wants you to either normalize the vectors prior to running them through the similarity function or switch to a different similarity metric. What should you do?\n",
    "\n",
    "\n",
    "#### 1. Create a list of all the **vocabulary $V$**\n",
    "\n",
    "Using **`sklearn`**'s **`CountVectorizer`**:\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too\", \n",
    "\"John also likes to watch football games, but he likes to watch movies on occasion as well\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "V = vectorizer.get_feature_names()\n",
    "```\n",
    "\n",
    "##### Native Implementation:\n",
    "```python\n",
    "def get_vocabulary(sentences):\n",
    "    vocabulary = {} # create an empty set - question: Why not a list?\n",
    "    for sentence in sentences:\n",
    "         # this is a very crude form of \"tokenization\", would not actually use in production\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.add(word)\n",
    "    return vocabulary\n",
    "```\n",
    "\n",
    "#### 2. Create your Bag of Words model\n",
    "```python\n",
    "X = X.toarray()\n",
    "print(X)\n",
    "```\n",
    "Your console output:\n",
    "```python\n",
    "[[0 0 0 1 2 1 2 1 1 1]\n",
    " [1 1 1 1 1 0 0 1 0 1]]\n",
    "```\n",
    "\n",
    "#### 3. Define your cosine similarity and Pearson correlation functions\n",
    "\n",
    "```python\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = dot(A, B)\n",
    "    denominator = norm(A) * norm(B)\n",
    "    return 1 - numerator / denominator\n",
    "\n",
    "A = [0,2,3,4,1,2]\n",
    "B = [1,3,4,0,0,2]\n",
    "\n",
    "# check that your native implementation and 3rd party library function produce the same values\n",
    "assert round(cosine_similarity(A,B),4) ==  round(cosine(A,B),4)\n",
    "```\n",
    "\n",
    "#### 4. Get the two documents from the BoW feature space and calculate cosine similarity\n",
    "\n",
    "```python\n",
    "cosine_similarity(X[0], X[1])\n",
    "```\n",
    ">0.5241424183609592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2494446500534866"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = dot(A, B)\n",
    "    denominator = norm(A) * norm(B)\n",
    "    return numerator / denominator # remember, you take 1 - the distance to get the distance\n",
    "\n",
    "A = [0,2,3,4,1,2]\n",
    "B = [1,3,4,0,0,2]\n",
    "\n",
    "# check that your native implementation and 3rd party library function produce the same values\n",
    "assert round(cosine_similarity(A,B),4) ==  round(1 - cosine(A,B),4)\n",
    "\n",
    "# check for shift invariance\n",
    "cosine(np.array(A), B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5241424183609592"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "X = X.toarray()\n",
    "print(vectorizer.get_feature_names())\n",
    "cosine_similarity(X[0], X[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "Pointwise mutual information measures the ratio between the **joint probability of two events happening** with the probabilities of the two events happening, assuming they are independent. It can be defined with the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "MI_{i,j} = log(\\frac{P(i,j)}{P(i)P(j)})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Remember that when two events are independent, $P(i,j) = P(i)P(j)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english') + [\".\",\",\",\":\", \"''\", \"'s\", \"'\", \"``\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "articles = [\n",
    "    \"bbcsport/football/001.txt\",\n",
    "    \"bbcsport/football/002.txt\",\n",
    "    \"bbcsport/football/003.txt\"\n",
    "]\n",
    "\n",
    "for article in articles:\n",
    "    article = open(article)\n",
    "    for line in article.readlines():\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        if len(line) > 0:\n",
    "\n",
    "            line = [lemmatizer.lemmatize(token) for token in word_tokenize(line)]\n",
    "            for word in line:\n",
    "                if word in stopwords:\n",
    "                    line.remove(word)\n",
    "            documents.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Manchester', 'United'),\n",
       " ('Van', 'Nistelrooy'),\n",
       " ('``', 'I'),\n",
       " ('said', '``'),\n",
       " ('``', 'He'),\n",
       " ('.', \"''\"),\n",
       " ('23', 'minute'),\n",
       " ('Alex', 'Ferguson'),\n",
       " ('But', 'wa'),\n",
       " ('City', 'Sunday')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collocation_finder = BigramCollocationFinder.from_documents(documents)\n",
    "measures = BigramAssocMeasures()\n",
    "\n",
    "collocation_finder.nbest(measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepts to Understand\n",
    "\n",
    "## Statistics\n",
    "- difference between similarity and distance\n",
    "- when a dot product versus a cross product is used\n",
    "- relationship between cosine distance and cosine similarity\n",
    "- orthogonality\n",
    "- Manhattan distance\n",
    "- Mahalanobis distance\n",
    "- Euclidean distance\n",
    "- Intersection, Union, Jaccard similarity\n",
    "- cosine similarity\n",
    "- pointwise mutual information\n",
    "- the impact of high-dimensional data on distance measures\n",
    "\n",
    "## Computer Science\n",
    "- CSR (compressed sparse row matrix)\n",
    "\n",
    "## Business / Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
