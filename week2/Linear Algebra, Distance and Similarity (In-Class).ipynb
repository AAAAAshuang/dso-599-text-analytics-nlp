{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "\n",
    "Links: https://www.fastcompany.com/90205359/google-you-auto-complete-me\n",
    "\n",
    "## Dot Products\n",
    "\n",
    "A dot product is defined as\n",
    "\n",
    "$ a \\cdot b = \\sum_{i}^{n} a_{i}b_{i} = a_{1}b_{1} + a_{2}b_{2} + a_{3}b_{3} + \\dots + a_{n}b_{n}$\n",
    "\n",
    "The geometric definition of a dot product is \n",
    "\n",
    "$ a \\cdot b = $\\|\\|b\\|\\|\\|\\|a\\|\\|\n",
    "\n",
    "### What does a dot product conceptually mean?\n",
    "\n",
    "A dot product is a representation of the similarity between two components, because it is calculated based upon shared elements.\n",
    "\n",
    "\n",
    "The actual value of a dot product reflects the direction of change:\n",
    "\n",
    "* **Zero**: we don't have any growth in the original direction\n",
    "* **Positive** number: we have some growth in the original direction\n",
    "* **Negative** number: we have negative (reverse) growth in the original direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.4480736161291701"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [0,2]\n",
    "B = [0,1]\n",
    "\n",
    "\n",
    "# What will the dot product of A and B be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [1,2]\n",
    "B = [2,4]\n",
    "# What will the dot product of A and B be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words Models\n",
    "\n",
    "You can use **`sklearn.feature_extraction.text.CountVectorizer`** to easily convert your corpus into a bag of words matrix:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "data_corpus = [\"John likes to watch movies. Mary likes movies too.\", \n",
    "\"John also likes to watch football games. Mary does not like football much.\"]\n",
    "X = vectorizer.fit_transform(data_corpus) \n",
    "```\n",
    "Note that the output `X` here is not your traditional Numpy matrix! Calling **`type(X)`** here will yield **`<class 'scipy.sparse.csr.csr_matrix'>`**, which is a **CSR ([compressed sparse row format matrix](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.sparse.csr_matrix.html))**. To convert it into an actual matrix, call the `toarray()` method:\n",
    "\n",
    "```python\n",
    "X.toarray()\n",
    "```\n",
    "Your output will be \n",
    "\n",
    "```\n",
    "array([[0, 0, 0, 0, 1, 0, 2, 1, 2, 0, 0, 1, 1, 1],\n",
    "       [1, 1, 2, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1]], dtype=int64)\n",
    "```\n",
    "Notice that using **`X.shape`** $\\rightarrow$ `(2,14)`, indicating a total vocabulary size $V$ of 14. To get what word each of the 14 columns corresponds to, use **`vectorizer.get_feature_names()`**:\n",
    "```\n",
    "['also', 'does', 'football', 'games', 'john', 'like', 'likes', 'mary', 'movies', 'much', 'not', 'to', 'too', 'watch']\n",
    "```\n",
    "\n",
    "Notice, however, that as the vocabulary size $V$ increases, the percent of the matrix taken up by zero values increases:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits.\"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bag of words feature space is 72.63% sparse. \n",
      "That's approximately 2777.34 bytes of wasted memory.\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "        \"Some analysts think demand could drop this year because a large number of homeowners take on remodeling projectsafter buying a new property. With fewer homes selling, home values easing, and mortgage rates rising, they predict home renovations could fall to their lowest levels in three years.\", \n",
    "    \n",
    "          \"Most home improvement stocks are expected to report fourth-quarter earnings next month.\",\n",
    "    \n",
    "         \"The conversation boils down to how much leverage management can get out of its wide-ranging efforts to re-energize operations, branding, digital capabilities, and the menu–and, for investors, how much to pay for that.\",\n",
    "    \n",
    "    \"RMD’s software acquisitions, efficiency, and mix overcame pricing and its gross margin improved by 90 bps Y/Y while its operating margin (including amortization) improved by 80 bps Y/Y. Since RMD expects the slower international flow generator growth to continue for the next few quarters, we have lowered our organic growth estimates to the mid-single digits. \"\n",
    "]\n",
    "\n",
    "X = vectorizer.fit_transform(corpus).toarray() \n",
    "import numpy as np\n",
    "from sys import getsizeof\n",
    "\n",
    "zeroes = np.where(X.flatten() == 0)[0].size \n",
    "percent_sparse = zeroes / X.size\n",
    "print(f\"The bag of words feature space is {round(percent_sparse * 100,2)}% sparse. \\n\\\n",
    "That's approximately {round(getsizeof(X) * percent_sparse,2)} bytes of wasted memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance Measures\n",
    "\n",
    "\n",
    "## Euclidean Distance\n",
    "\n",
    "Euclidean distances can range from 0 (completely identically) to $\\infty$ (extremely dissimilar). **Magnitude** plays an extremely important role:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    " \n",
    "# Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's typically an easier way to write this function that takes advantage of Numpy's vectorization capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Method 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Measures\n",
    "\n",
    "Similarity measures will always range between -1 and 1. A similarity of -1 means the two objects are complete opposites, while a similarity of 1 indicates the objects are identical.\n",
    "\n",
    "## Pearson Correlation Coefficient\n",
    "* We use **ρ** when the correlation is being measured from the population, and **r** when it is being generated from a sample.\n",
    "* An r value of 1 represents a **perfect linear** relationship, and a value of -1 represents a perfect inverse linear relationship.\n",
    "\n",
    "The equation for Pearson's correlation coefficient is \n",
    "$$\n",
    "ρ_{Χ_Υ} = \\frac{cov(X,Y)}{σ_Xσ_Y}\n",
    "$$\n",
    "\n",
    "### Intuition Behind Pearson Correlation Coefficient\n",
    "\n",
    "#### When $ρ_{Χ_Υ} = 1$ or  $ρ_{Χ_Υ} = -1$\n",
    "\n",
    "This requires **$cov(X,Y) = σ_Xσ_Y$** or **$-1 * cov(X,Y) = σ_Xσ_Y$** (in the case of $ρ = -1$) . This corresponds with all the data points lying perfectly on the same line.\n",
    "![Correlations](images/correlation.png \"Visualization of various r values for Pearson correlation coefficient\")\n",
    "\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "The cosine similarity of two vectors (each vector will usually represent one document) is a measure that calculates $ cos(\\theta)$, where $\\theta$ is the angle between the two vectors.\n",
    "\n",
    "Therefore, if the vectors are **orthogonal** to each other (90 degrees), $cos(90) = 0$. If the vectors are in exactly the same direction, $\\theta = 0$ and $cos(0) = 1$.\n",
    "\n",
    "Cosine similiarity **does not care about the magnitude of the vector, only the direction** in which it points. This can help normalize when comparing across documents that are different in terms of word count.\n",
    "\n",
    "### Shift Invariance\n",
    "\n",
    "* The Pearson correlation coefficient between X and Y does not change with you transform $X \\rightarrow a + bX$ and $Y \\rightarrow c + dY$, assuming $a$, $b$, $c$, and $d$ are constants and $b$ and $d$ are positive.\n",
    "* Cosine similarity does, however, change when transformed in this way.\n",
    "\n",
    "\n",
    "<h1><span style=\"background-color: #FFFF00\">Exercise (10 minutes):</span></h1>\n",
    "\n",
    ">In Python, find the **cosine similarity** of the two following sentences, assuming a **bag of words** model. You may use a library to create the BoW feature space, but do not use libraries other than `numpy` or `scipy` to compute Pearson and and cosine similarity:\n",
    "\n",
    ">`A = \"John likes to watch movies. Mary likes movies too\"`\n",
    "\n",
    ">`B = \"John also likes to watch football games, but he likes to watch movies on occasion as well\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pointwise Mutual Information\n",
    "\n",
    "Pointwise mutual information measures the ratio between the **joint probability of two events happening** with the probabilities of the two events happening, assuming they are independent. It can be defined with the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "MI_{i,j} = log(\\frac{P(i,j)}{P(i)P(j)})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Remember that when two events are independent, $P(i,j) = P(i)P(j)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
